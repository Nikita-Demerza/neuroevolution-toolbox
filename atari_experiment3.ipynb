{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iV7GFD8QRQVe"
   },
   "outputs": [],
   "source": [
    "#!pip3 install \"gym[atari]\"\n",
    "import gym\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neural_tape_controller\n",
    "#Положительные числа - положительные награды.\n",
    "import tasks \n",
    "import optimize\n",
    "import  pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gK_0udONWWcN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Max Episode Steps: 27000\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Range: {env.reward_range}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "query_environment(\"ALE/Tetris-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yHqp2lahT3dG"
   },
   "outputs": [],
   "source": [
    "def encode_simple(X):\n",
    "  #как сделать простое кодирование? Ну давай разобьём картинку двумя разными сетками и измерим яркость\n",
    "  #на входе одиночная картинка\n",
    "  y_lst = []\n",
    "  sz = np.shape(X)\n",
    "  count_squares = 12\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  count_squares = 9\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  count_squares = 5\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  #count_squares = 2\n",
    "  #for i in range(count_squares):\n",
    "  #  for j in range(count_squares):\n",
    "  #    x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "  #    y_lst.append(np.nanmean(x))\n",
    "  #размер: 62\n",
    "  return np.array(y_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_hxFWNWoOsmJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded successfully\n"
     ]
    }
   ],
   "source": [
    "n_actions = 5\n",
    "embed_size = 250#это эмбеддинг картинки\n",
    "nt = neural_tape_controller.nt_controller(input_size=embed_size,output_size=n_actions)\n",
    "try:\n",
    "    with open('./genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "        genom = pickle.load(f)\n",
    "        genom = genom[-1]\n",
    "    print('loaded successfully')\n",
    "except Exception:\n",
    "    genom = nt.nn.disassemble_genom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3, 1, 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 5\n",
    "embed_size = 100800#это эмбеддинг картинки\n",
    "nt = neural_tape_controller.nt_controller(tacts=1,input_size=embed_size,output_size=n_actions)\n",
    "genom = nt.nn.disassemble_genom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bXF5TTLgIheU"
   },
   "outputs": [],
   "source": [
    "def test_atary(genom, draw=False,game_name=\"ALE/Tetris-v5\",seed=0,controller=None):\n",
    "\n",
    "  env = gym.make(game_name)\n",
    "  env.reset()\n",
    "  n_actions = env.action_space.n\n",
    "  state_dim = env.observation_space.shape#здесь картинка\n",
    "  #print('n_actions',n_actions,'state_dim',state_dim)\n",
    "  #1/0\n",
    "  state_dim = embed_size#это эмбеддинг картинки\n",
    "  if controller==None: controller = neural_tape_controller.nt_controller(tacts=1,genom=np.array(genom),input_size=state_dim,output_size=n_actions) \n",
    "  out_tape = np.zeros(30)\n",
    "  reward_sum = 0\n",
    "\n",
    "  #seed=1\n",
    "  #np.random.seed(seed)\n",
    "  #env.seed(seed)\n",
    "  while True:\n",
    "  #for i in range(200):\n",
    "      action = np.ravel(out_tape)\n",
    "      action += np.random.rand(len(action))*np.std(action)*0.05#игра детерминистическая, иначе рандома не будет\n",
    "      #print('action',int(np.argmax(action)),action)\n",
    "      #исполнить env\n",
    "      state, reward, done,_ = env.step(int(np.argmax(action)))\n",
    "        \n",
    "      shp = np.shape(state)\n",
    "      #print(shp)\n",
    "      #1/0\n",
    "      state = np.reshape(state,[shp[2],shp[0],shp[1]])\n",
    "      #print(np.shape(state))\n",
    "        \n",
    "      t=pd.Timestamp.now()\n",
    "      if done:\n",
    "        break\n",
    "      #state = encode_mobnet(np.array([state]))\n",
    "      #state = encode_simple(state)\n",
    "      reward_sum += (reward + 0.1)\n",
    "      out_tape = controller.act(torch.tensor(state),reward,done)\n",
    "      if draw:\n",
    "        globals()['video'].append(Image.fromarray(env.render(\"rgb_array\")))\n",
    "  return reward_sum, controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jn_DW2_xYhb_"
   },
   "outputs": [],
   "source": [
    "def multy_test(genom,draw=False):\n",
    "  if draw:\n",
    "    globals()['video'] = []\n",
    "\n",
    "  game_name = \"ALE/Tetris-v5\"\n",
    "  q_arr = []\n",
    "  controller = neural_tape_controller.nt_controller(tacts=1,genom=np.array(genom),input_size=100800,output_size=n_actions) \n",
    "  for i in range(3):\n",
    "    q,controller = test_atary(genom, draw=draw,game_name=game_name,controller=controller)\n",
    "    q_arr.append(q*(i+1))\n",
    "  return np.sum(q_arr)/len(q_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2bNLh6kmFzfY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 56.50000000000053\n",
      "CPU times: user 1min 27s, sys: 48.9 ms, total: 1min 27s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q,_=test_atary(genom,game_name=\"ALE/Tetris-v5\", draw=False)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cGZxP_NhdC27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 121.93333333333449\n",
      "CPU times: user 5min 50s, sys: 340 ms, total: 5min 50s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q=multy_test(genom)\n",
    "print('q=',q)\n",
    "#q=multy_test(genom)\n",
    "#print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Eja2m2P_ewzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= (69.70000000000032, <neural_tape_controller.nt_controller object at 0x7f2ddccad9a0>)\n",
      "CPU times: user 3min 53s, sys: 269 ms, total: 3min 53s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q=test_atary(genom)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oN_h1Rg_2fQX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 19:50:32.940569\n"
     ]
    }
   ],
   "source": [
    "print(pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "history loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "opt = optimize.optimizer(multy_test, genom_size=len(genom),parallel_cores=1,init_file='genoms/genom1.pkl',history_file='history/history2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "        opt.best_genoms = pickle.load(f)\n",
    "    print('loaded successfully')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HzpNigFUTmIM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt# 0\n",
      "scores for optimizers augmented [-8.00450000e+01  3.26644167e+02  1.01664167e+03  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  1.00000000e+10]\n",
      "chosen evol_narrow previous_result: nan per tacts: nan\n",
      "iteration 0 y= [115.  111.8]\n",
      "iteration 1 y= [120.63333333 117.1       ]\n",
      "iteration 2 y= [128.9        122.86666667]\n",
      "iteration 3 y= [126.5 125. ]\n",
      "iteration 4 y= [130.76666667 118.4       ]\n",
      "iteration 5 y= [117.56666667 117.4       ]\n",
      "iteration 6 y= [123.6        123.03333333]\n",
      "iteration 7 y= [126.2        120.43333333]\n",
      "iteration 8 y= [126.83333333 123.1       ]\n",
      "iteration 9 y= [123.2 119.9]\n",
      "iteration 10 y= [127.93333333 123.9       ]\n",
      "iteration 11 y= [132.         118.46666667]\n",
      "iteration 12 y= [122.06666667 120.26666667]\n",
      "iteration 13 y= [136.43333333 115.53333333]\n",
      "iteration 14 y= [133.46666667 133.13333333]\n",
      "iteration 15 y= [136.76666667 119.63333333]\n",
      "iteration final y= [138.6        135.46666667]\n",
      "result evol_narrow previous_gain: 23.59999999999893 per tacts: 136 duration 0 days 03:32:28.255864\n",
      "WRITTEN\n",
      "opt# 1\n",
      "scores for optimizers augmented [-8.00450000e+01  3.26644167e+02  1.01664167e+03  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  2.35320000e+01]\n",
      "chosen rel_coord_default previous_result: 23.59999999999893 per tacts: 136\n",
      "score_new 123.20000000000118 score_prev 99.13333333333419 gained 24.06666666666699\n",
      "score_new 108.76666666666756 score_prev 123.20000000000118 gained -14.433333333333621\n",
      "undo\n",
      "score_new 128.1666666666677 score_prev 123.20000000000118 gained 4.9666666666665265\n",
      "score_new 124.5333333333344 score_prev 128.1666666666677 gained -3.6333333333333115\n",
      "undo\n",
      "score_new 92.2333333333341 score_prev 128.1666666666677 gained -35.93333333333361\n",
      "undo\n",
      "score_new 102.53333333333426 score_prev 128.1666666666677 gained -25.633333333333454\n",
      "undo\n",
      "score_new 128.56666666666766 score_prev 128.1666666666677 gained 0.39999999999994884\n",
      "score_new 108.16666666666765 score_prev 128.56666666666766 gained -20.400000000000006\n",
      "undo\n",
      "score_new 139.23333333333395 score_prev 128.56666666666766 gained 10.666666666666288\n",
      "score_new 121.70000000000108 score_prev 139.23333333333395 gained -17.533333333332862\n",
      "undo\n",
      "score_new 103.6666666666676 score_prev 139.23333333333395 gained -35.56666666666635\n",
      "undo\n",
      "score_new 100.06666666666756 score_prev 139.23333333333395 gained -39.16666666666639\n",
      "undo\n",
      "score_new 122.53333333333337 score_prev 139.23333333333395 gained -16.70000000000057\n",
      "undo\n",
      "score_new 97.20000000000084 score_prev 139.23333333333395 gained -42.033333333333104\n",
      "undo\n",
      "score_new 106.80000000000086 score_prev 139.23333333333395 gained -32.43333333333308\n",
      "undo\n",
      "score_new 98.06666666666752 score_prev 139.23333333333395 gained -41.16666666666643\n",
      "undo\n",
      "score_new 79.56666666666725 score_prev 139.23333333333395 gained -59.6666666666667\n",
      "undo\n",
      "score_new 126.8000000000002 score_prev 139.23333333333395 gained -12.43333333333375\n",
      "undo\n",
      "score_new 113.26666666666773 score_prev 139.23333333333395 gained -25.966666666666214\n",
      "undo\n",
      "score_new 104.16666666666761 score_prev 139.23333333333395 gained -35.066666666666336\n",
      "undo\n",
      "score_new 86.46666666666734 score_prev 139.23333333333395 gained -52.76666666666661\n",
      "undo\n",
      "score_new 107.66666666666765 score_prev 139.23333333333395 gained -31.566666666666293\n",
      "undo\n",
      "score_new 101.13333333333424 score_prev 139.23333333333395 gained -38.09999999999971\n",
      "undo\n",
      "score_new 117.00000000000068 score_prev 139.23333333333395 gained -22.233333333333263\n",
      "undo\n",
      "score_new 117.70000000000113 score_prev 139.23333333333395 gained -21.53333333333282\n",
      "undo\n",
      "result rel_coord_default previous_gain: 23.59999999999893 per tacts: 136 duration 0 days 00:43:20.370111\n",
      "WRITTEN\n",
      "opt# 2\n",
      "scores for optimizers augmented [-8.00450000e+01  3.26644167e+02  5.28358333e+02  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  2.35320000e+01]\n",
      "chosen rel_coord_default previous_result: 23.59999999999893 per tacts: 136\n",
      "score_new 93.26666666666745 score_prev 92.00000000000075 gained 1.2666666666666941\n",
      "score_new 109.96666666666749 score_prev 93.26666666666745 gained 16.700000000000045\n",
      "score_new 116.4666666666677 score_prev 109.96666666666749 gained 6.500000000000213\n",
      "score_new 117.60000000000112 score_prev 116.4666666666677 gained 1.133333333333411\n",
      "score_new 119.83333333333451 score_prev 117.60000000000112 gained 2.233333333333391\n",
      "score_new 103.40000000000093 score_prev 119.83333333333451 gained -16.43333333333358\n",
      "undo\n",
      "score_new 113.16666666666761 score_prev 119.83333333333451 gained -6.666666666666899\n",
      "undo\n",
      "score_new 113.26666666666773 score_prev 119.83333333333451 gained -6.566666666666777\n",
      "undo\n",
      "score_new 130.13333333333438 score_prev 119.83333333333451 gained 10.29999999999987\n",
      "score_new 109.93333333333436 score_prev 130.13333333333438 gained -20.200000000000017\n",
      "undo\n",
      "score_new 125.50000000000095 score_prev 130.13333333333438 gained -4.633333333333425\n",
      "undo\n",
      "score_new 108.400000000001 score_prev 130.13333333333438 gained -21.733333333333377\n",
      "undo\n",
      "score_new 119.36666666666781 score_prev 130.13333333333438 gained -10.766666666666566\n",
      "undo\n",
      "score_new 90.86666666666741 score_prev 130.13333333333438 gained -39.266666666666964\n",
      "undo\n",
      "score_new 133.8000000000004 score_prev 130.13333333333438 gained 3.666666666666032\n",
      "score_new 103.00000000000091 score_prev 133.8000000000004 gained -30.7999999999995\n",
      "undo\n",
      "score_new 101.53333333333423 score_prev 133.8000000000004 gained -32.26666666666618\n",
      "undo\n",
      "score_new 114.53333333333414 score_prev 133.8000000000004 gained -19.266666666666268\n",
      "undo\n",
      "score_new 118.06666666666747 score_prev 133.8000000000004 gained -15.733333333332936\n",
      "undo\n",
      "score_new 108.33333333333428 score_prev 133.8000000000004 gained -25.46666666666613\n",
      "undo\n",
      "score_new 89.63333333333406 score_prev 133.8000000000004 gained -44.166666666666345\n",
      "undo\n",
      "score_new 127.43333333333446 score_prev 133.8000000000004 gained -6.3666666666659495\n",
      "undo\n",
      "score_new 83.90000000000066 score_prev 133.8000000000004 gained -49.89999999999975\n",
      "undo\n",
      "score_new 110.93333333333415 score_prev 133.8000000000004 gained -22.866666666666262\n",
      "undo\n",
      "score_new 75.43333333333386 score_prev 133.8000000000004 gained -58.366666666666546\n",
      "undo\n",
      "result rel_coord_default previous_gain: 23.59999999999893 per tacts: 136 duration 0 days 00:41:52.511940\n",
      "WRITTEN\n",
      "opt# 3\n",
      "scores for optimizers augmented [-8.00450000e+01  3.26644167e+02  3.66163889e+02  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  2.35320000e+01]\n",
      "chosen rel_coord_default previous_result: 23.59999999999893 per tacts: 136\n",
      "score_new 111.13333333333419 score_prev 96.33333333333417 gained 14.800000000000026\n",
      "score_new 113.13333333333405 score_prev 111.13333333333419 gained 1.999999999999858\n",
      "score_new 83.60000000000065 score_prev 113.13333333333405 gained -29.533333333333402\n",
      "undo\n",
      "score_new 100.73333333333422 score_prev 113.13333333333405 gained -12.399999999999835\n",
      "undo\n",
      "score_new 102.06666666666757 score_prev 113.13333333333405 gained -11.066666666666478\n",
      "undo\n",
      "score_new 121.96666666666742 score_prev 113.13333333333405 gained 8.833333333333371\n",
      "score_new 102.80000000000092 score_prev 121.96666666666742 gained -19.1666666666665\n",
      "undo\n",
      "score_new 97.1666666666675 score_prev 121.96666666666742 gained -24.799999999999926\n",
      "undo\n",
      "score_new 131.13333333333424 score_prev 121.96666666666742 gained 9.166666666666814\n",
      "score_new 126.90000000000077 score_prev 131.13333333333424 gained -4.233333333333462\n",
      "undo\n",
      "score_new 111.73333333333397 score_prev 131.13333333333424 gained -19.40000000000026\n",
      "undo\n",
      "score_new 106.00000000000097 score_prev 131.13333333333424 gained -25.13333333333327\n",
      "undo\n",
      "score_new 125.1000000000012 score_prev 131.13333333333424 gained -6.033333333333033\n",
      "undo\n",
      "score_new 96.06666666666747 score_prev 131.13333333333424 gained -35.06666666666676\n",
      "undo\n",
      "score_new 105.53333333333428 score_prev 131.13333333333424 gained -25.59999999999995\n",
      "undo\n",
      "score_new 130.60000000000068 score_prev 131.13333333333424 gained -0.5333333333335588\n",
      "undo\n",
      "score_new 100.10000000000089 score_prev 131.13333333333424 gained -31.033333333333346\n",
      "undo\n",
      "score_new 100.00000000000087 score_prev 131.13333333333424 gained -31.13333333333337\n",
      "undo\n",
      "score_new 97.70000000000051 score_prev 131.13333333333424 gained -33.43333333333372\n",
      "undo\n",
      "score_new 115.66666666666775 score_prev 131.13333333333424 gained -15.466666666666484\n",
      "undo\n",
      "score_new 109.80000000000098 score_prev 131.13333333333424 gained -21.333333333333258\n",
      "undo\n",
      "score_new 129.13333333333378 score_prev 131.13333333333424 gained -2.0000000000004547\n",
      "undo\n",
      "score_new 127.30000000000102 score_prev 131.13333333333424 gained -3.833333333333215\n",
      "undo\n",
      "score_new 114.36666666666775 score_prev 131.13333333333424 gained -16.76666666666648\n",
      "undo\n",
      "score_new 100.36666666666736 score_prev 131.13333333333424 gained -30.76666666666688\n",
      "undo\n",
      "result rel_coord_default previous_gain: 23.59999999999893 per tacts: 136 duration 0 days 00:44:12.965511\n",
      "WRITTEN\n",
      "opt# 4\n",
      "scores for optimizers augmented [-8.00450000e+01  3.26644167e+02  2.83316667e+02  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  2.35320000e+01]\n",
      "chosen gradient_wide_50 previous_result: 23.59999999999893 per tacts: 136\n",
      "score_new 119.40000000000032 score_prev 88.93333333333406 gained 30.466666666666256\n",
      "score_new 111.70000000000103 score_prev 119.40000000000032 gained -7.699999999999292\n",
      "but success with one derivative: score_new 112.40000000000101 score_prev 119.40000000000032 gained -6.999999999999304\n",
      "undo\n",
      "score_new 106.60000000000097 score_prev 119.40000000000032 gained -12.799999999999343\n",
      "but success with one derivative: score_new 108.86666666666758 score_prev 119.40000000000032 gained -10.533333333332735\n",
      "undo\n",
      "score_new 101.53333333333423 score_prev 119.40000000000032 gained -17.86666666666609\n",
      "but success with one derivative: score_new 108.200000000001 score_prev 119.40000000000032 gained -11.19999999999932\n",
      "undo\n",
      "score_new 107.46666666666756 score_prev 119.40000000000032 gained -11.933333333332754\n",
      "but success with one derivative: score_new 93.20000000000078 score_prev 119.40000000000032 gained -26.199999999999534\n",
      "undo\n",
      "result gradient_wide_50 previous_gain: 23.59999999999893 per tacts: 136 duration 0 days 00:49:28.976173\n",
      "WRITTEN\n",
      "opt# 5\n",
      "scores for optimizers augmented [-8.00450000e+01  1.78544167e+02  2.83316667e+02  1.65200000e+00\n",
      "  2.22666667e+00 -4.20000000e-02 -7.50000000e-02  6.66666667e-02\n",
      "  2.35320000e+01]\n",
      "chosen rel_coord_default previous_result: 23.59999999999893 per tacts: 136\n",
      "score_new 115.7000000000009 score_prev 107.63333333333433 gained 8.066666666666563\n",
      "score_new 98.86666666666753 score_prev 115.7000000000009 gained -16.83333333333337\n",
      "undo\n",
      "score_new 116.46666666666778 score_prev 115.7000000000009 gained 0.7666666666668789\n",
      "score_new 98.93333333333419 score_prev 116.46666666666778 gained -17.533333333333587\n",
      "undo\n",
      "score_new 108.2666666666676 score_prev 116.46666666666778 gained -8.200000000000173\n",
      "undo\n",
      "score_new 112.30000000000105 score_prev 116.46666666666778 gained -4.166666666666728\n",
      "undo\n",
      "score_new 123.50000000000114 score_prev 116.46666666666778 gained 7.03333333333336\n",
      "score_new 92.43333333333412 score_prev 123.50000000000114 gained -31.066666666667018\n",
      "undo\n",
      "score_new 86.30000000000068 score_prev 123.50000000000114 gained -37.20000000000046\n",
      "undo\n",
      "score_new 92.2000000000005 score_prev 123.50000000000114 gained -31.300000000000637\n",
      "undo\n",
      "score_new 135.1333333333338 score_prev 123.50000000000114 gained 11.633333333332672\n",
      "score_new 84.36666666666733 score_prev 135.1333333333338 gained -50.76666666666648\n",
      "undo\n",
      "score_new 108.06666666666767 score_prev 135.1333333333338 gained -27.066666666666137\n",
      "undo\n",
      "score_new 103.33333333333427 score_prev 135.1333333333338 gained -31.799999999999542\n",
      "undo\n",
      "score_new 97.6666666666675 score_prev 135.1333333333338 gained -37.46666666666631\n",
      "undo\n",
      "score_new 89.26666666666739 score_prev 135.1333333333338 gained -45.86666666666642\n",
      "undo\n",
      "score_new 97.10000000000083 score_prev 135.1333333333338 gained -38.033333333332976\n",
      "undo\n",
      "score_new 110.46666666666768 score_prev 135.1333333333338 gained -24.66666666666613\n",
      "undo\n",
      "score_new 85.83333333333401 score_prev 135.1333333333338 gained -49.2999999999998\n",
      "undo\n",
      "score_new 123.4333333333343 score_prev 135.1333333333338 gained -11.699999999999505\n",
      "undo\n",
      "score_new 92.5333333333341 score_prev 135.1333333333338 gained -42.59999999999971\n",
      "undo\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/.private/nikita/ipykernel_4653/1451368021.py\", line 5, in <cell line: 3>\n",
      "    opt.optimize()\n",
      "  File \"/home/nikita/neuroevolution-toolbox/optimize.py\", line 88, in optimize\n",
      "    elif chosen_optimizer=='gradient_wide_50':\n",
      "  File \"/home/nikita/neuroevolution-toolbox/optimize.py\", line 331, in rel_coord_default\n",
      "  File \"/home/nikita/neuroevolution-toolbox/optimize.py\", line 358, in relative_coordinate_descent\n",
      "    genom_local[idx:idx+stripe] *= 1+step\n",
      "  File \"/tmp/.private/nikita/ipykernel_4653/4178496395.py\", line 9, in multy_test\n",
      "    q,controller = test_atary(genom, draw=draw,game_name=game_name,controller=controller)\n",
      "  File \"/tmp/.private/nikita/ipykernel_4653/3154230333.py\", line 37, in test_atary\n",
      "    out_tape = controller.act(state,reward,done)\n",
      "  File \"/home/nikita/neuroevolution-toolbox/neural_tape_controller.py\", line 97, in act\n",
      "  File \"/home/nikita/neuroevolution-toolbox/nnet.py\", line 406, in predict_vector\n",
      "    if torch.sum(torch.isnan(y))>0:\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/interactiveshell.py\", line 1992, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/nikita/.local/lib/python3/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.function(opt.best_genoms[-1])\n",
    "\n",
    "for i in range(1000):\n",
    "    print('opt#',i)\n",
    "    opt.optimize()\n",
    "    with open('genoms/best_genom_tetris.pkl', 'wb') as f:\n",
    "        pickle.dump(opt.best_genoms,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('WRITTEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./genoms/genom1.pkl', 'rb') as f:\n",
    "        genom = pickle.load(f)\n",
    "        #genom = genom[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LBhHJSL-ctuu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 90.86666666666578\n"
     ]
    }
   ],
   "source": [
    "q=multy_test(genom,draw=True)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rquJJEbVcfgz"
   },
   "outputs": [],
   "source": [
    "video[0].save(\n",
    "    './out_videos/tetris.gif',\n",
    "    save_all=True,\n",
    "    append_images=video[1:], \n",
    "    optimize=True,\n",
    "    duration=100,\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise stategy\n",
    "#with open('genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "    #genom = pickle.load(f)[-1]\n",
    "globals()['video'] = []\n",
    "noise = np.random.normal(size=[len(genom)])*1\n",
    "for i in range(1):\n",
    "    genom = genom+noise\n",
    "    q=test_atary(genom, draw=True,game_name=\"ALE/Tetris-v5\",seed=0)\n",
    "    np.random.seed(i)\n",
    "    noise = np.random.normal(size=[len(genom)])*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NolxDC4bdi7r"
   },
   "outputs": [],
   "source": [
    "video[0].save(\n",
    "    './out_videos/tetris.gif',\n",
    "    save_all=True,\n",
    "    append_images=video[1:], \n",
    "    optimize=True,\n",
    "    duration=10,\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genoms/best_genom_tetris.pkl', 'wb') as f:\n",
    "    pickle.dump(genom,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('WRITTEN')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1/6tnER+ipw9TdwDyZ+sz",
   "collapsed_sections": [],
   "mount_file_id": "1LelKGSZ7B8o8qafqCX4oFBtzHByVJ9FI",
   "name": "atari_experiment2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
