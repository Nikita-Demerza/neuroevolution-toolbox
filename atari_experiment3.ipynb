{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iV7GFD8QRQVe"
   },
   "outputs": [],
   "source": [
    "#!pip3 install \"gym[atari]\"\n",
    "import gym\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neural_tape_controller\n",
    "#Положительные числа - положительные награды.\n",
    "import tasks \n",
    "import optimize\n",
    "import  pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gK_0udONWWcN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Max Episode Steps: 27000\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Range: {env.reward_range}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "query_environment(\"ALE/Tetris-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yHqp2lahT3dG"
   },
   "outputs": [],
   "source": [
    "def encode_simple(X):\n",
    "  #как сделать простое кодирование? Ну давай разобьём картинку двумя разными сетками и измерим яркость\n",
    "  #на входе одиночная картинка\n",
    "  y_lst = []\n",
    "  sz = np.shape(X)\n",
    "  count_squares = 12\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  count_squares = 9\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  count_squares = 5\n",
    "  for i in range(count_squares):\n",
    "    for j in range(count_squares):\n",
    "      x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "      y_lst.append(np.nanmean(x))\n",
    "\n",
    "  #count_squares = 2\n",
    "  #for i in range(count_squares):\n",
    "  #  for j in range(count_squares):\n",
    "  #    x = X[int(i*sz[0]/count_squares):int((i+1)*sz[0]/count_squares), int(j*sz[1]/count_squares):int((j+1)*sz[1]/count_squares)]\n",
    "  #    y_lst.append(np.nanmean(x))\n",
    "  #размер: 62\n",
    "  return np.array(y_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_hxFWNWoOsmJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded successfully\n"
     ]
    }
   ],
   "source": [
    "n_actions = 5\n",
    "embed_size = 250#это эмбеддинг картинки\n",
    "nt = neural_tape_controller.nt_controller(input_size=embed_size,output_size=n_actions)\n",
    "try:\n",
    "    with open('./genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "        genom = pickle.load(f)\n",
    "        genom = genom[-1]\n",
    "    print('loaded successfully')\n",
    "except Exception:\n",
    "    genom = nt.nn.disassemble_genom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3, 1, 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 5\n",
    "embed_size = 100800#это эмбеддинг картинки\n",
    "nt = neural_tape_controller.nt_controller(tacts=1,input_size=embed_size,output_size=n_actions)\n",
    "genom = nt.nn.disassemble_genom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bXF5TTLgIheU"
   },
   "outputs": [],
   "source": [
    "def test_atary(genom, draw=False,game_name=\"ALE/Tetris-v5\",seed=0,controller=None):\n",
    "\n",
    "  env = gym.make(game_name)\n",
    "  env.reset()\n",
    "  n_actions = env.action_space.n\n",
    "  state_dim = env.observation_space.shape#здесь картинка\n",
    "  #print('n_actions',n_actions,'state_dim',state_dim)\n",
    "  #1/0\n",
    "  state_dim = embed_size#это эмбеддинг картинки\n",
    "  if controller==None: controller = neural_tape_controller.nt_controller(tacts=1,genom=np.array(genom),input_size=state_dim,output_size=n_actions) \n",
    "  out_tape = np.zeros(30)\n",
    "  reward_sum = 0\n",
    "\n",
    "  #seed=1\n",
    "  #np.random.seed(seed)\n",
    "  #env.seed(seed)\n",
    "  while True:\n",
    "  #for i in range(200):\n",
    "      action = np.ravel(out_tape)\n",
    "      action += np.random.rand(len(action))*np.std(action)*0.05#игра детерминистическая, иначе рандома не будет\n",
    "      #print('action',int(np.argmax(action)),action)\n",
    "      #исполнить env\n",
    "      state, reward, done,_ = env.step(int(np.argmax(action)))\n",
    "        \n",
    "      shp = np.shape(state)\n",
    "      #print(shp)\n",
    "      #1/0\n",
    "      state = np.reshape(state,[shp[2],shp[0],shp[1]])\n",
    "      #print(np.shape(state))\n",
    "        \n",
    "      t=pd.Timestamp.now()\n",
    "      if done:\n",
    "        break\n",
    "      #state = encode_mobnet(np.array([state]))\n",
    "      #state = encode_simple(state)\n",
    "      reward_sum += (reward + 0.1)\n",
    "      out_tape = controller.act(torch.tensor(state),reward,done)\n",
    "      if draw:\n",
    "        globals()['video'].append(Image.fromarray(env.render(\"rgb_array\")))\n",
    "  return reward_sum, controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jn_DW2_xYhb_"
   },
   "outputs": [],
   "source": [
    "def multy_test(genom,draw=False):\n",
    "  if draw:\n",
    "    globals()['video'] = []\n",
    "\n",
    "  game_name = \"ALE/Tetris-v5\"\n",
    "  q_arr = []\n",
    "  controller = neural_tape_controller.nt_controller(tacts=1,genom=np.array(genom),input_size=100800,output_size=n_actions) \n",
    "  for i in range(3):\n",
    "    q,controller = test_atary(genom, draw=draw,game_name=game_name,controller=controller)\n",
    "    q_arr.append(q*(i+1))\n",
    "  return np.sum(q_arr)/len(q_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2bNLh6kmFzfY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 41.70000000000032\n",
      "CPU times: user 3.27 s, sys: 7.58 ms, total: 3.28 s\n",
      "Wall time: 630 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q,_=test_atary(genom,game_name=\"ALE/Tetris-v5\", draw=False)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cGZxP_NhdC27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 83.40000000000065\n",
      "CPU times: user 10.2 s, sys: 5.88 ms, total: 10.2 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q=multy_test(genom)\n",
    "print('q=',q)\n",
    "#q=multy_test(genom)\n",
    "#print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Eja2m2P_ewzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= (41.70000000000032, <neural_tape_controller.nt_controller object at 0x7f2f038451f0>)\n",
      "CPU times: user 3.26 s, sys: 3.27 ms, total: 3.26 s\n",
      "Wall time: 627 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q=test_atary(genom)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oN_h1Rg_2fQX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:56:26.443397\n"
     ]
    }
   ],
   "source": [
    "print(pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loaded successfully\n",
      "history loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "opt = optimize.optimizer(multy_test, genom_size=len(genom),parallel_cores=1,init_file='genoms/genom1.pkl',history_file='history/history.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "        opt.best_genoms = pickle.load(f)\n",
    "    print('loaded successfully')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HzpNigFUTmIM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt# 0\n",
      "scores for optimizers augmented tensor([-4.6445e+01,  1.0000e+10,  1.0000e+10,  1.0000e+10, -4.0000e-02,\n",
      "         1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10])\n",
      "chosen gradient_wide_50 previous_result: nan per tacts: nan\n",
      "score_new 112.20000000000105 score_prev 109.00000000000101 gained 3.2000000000000455\n",
      "score_new 111.13333333333436 score_prev 112.20000000000105 gained -1.0666666666666913\n",
      "but success with one derivative: score_new 95.80000000000082 score_prev 112.20000000000105 gained -16.400000000000233\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 112.20000000000105 gained -3.2000000000000455\n",
      "but success with one derivative: score_new 140.06666666666658 score_prev 112.20000000000105 gained 27.866666666665523\n",
      "score_new 108.600000000001 score_prev 140.06666666666658 gained -31.466666666665574\n",
      "but success with one derivative: score_new 109.00000000000101 score_prev 140.06666666666658 gained -31.06666666666557\n",
      "undo\n",
      "score_new 115.26666666666775 score_prev 140.06666666666658 gained -24.799999999998832\n",
      "but success with one derivative: score_new 109.00000000000101 score_prev 140.06666666666658 gained -31.06666666666557\n",
      "undo\n",
      "result gradient_wide_50 previous_gain: nan per tacts: nan duration 0 days 00:02:44.900624\n",
      "WRITTEN\n",
      "opt# 1\n",
      "scores for optimizers augmented tensor([-4.6445e+01,  3.1044e+01,  1.0000e+10,  1.0000e+10, -4.0000e-02,\n",
      "         1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10])\n",
      "chosen rel_coord_default previous_result: nan per tacts: nan\n",
      "score_new 103.53333333333426 score_prev 96.33333333333417 gained 7.200000000000088\n",
      "score_new 121.66666666666721 score_prev 103.53333333333426 gained 18.133333333332956\n",
      "score_new 109.00000000000101 score_prev 121.66666666666721 gained -12.666666666666202\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 121.66666666666721 gained -12.666666666666202\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 121.66666666666721 gained -12.666666666666202\n",
      "undo\n",
      "score_new 103.93333333333426 score_prev 121.66666666666721 gained -17.73333333333295\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 121.66666666666721 gained -12.666666666666202\n",
      "undo\n",
      "score_new 110.20000000000103 score_prev 121.66666666666721 gained -11.466666666666185\n",
      "undo\n",
      "score_new 122.06666666666739 score_prev 121.66666666666721 gained 0.4000000000001762\n",
      "score_new 101.40000000000089 score_prev 122.06666666666739 gained -20.6666666666665\n",
      "undo\n",
      "score_new 107.40000000000099 score_prev 122.06666666666739 gained -14.666666666666401\n",
      "undo\n",
      "score_new 110.8666666666677 score_prev 122.06666666666739 gained -11.19999999999969\n",
      "undo\n",
      "score_new 101.8000000000009 score_prev 122.06666666666739 gained -20.26666666666648\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 122.06666666666739 gained -13.066666666666379\n",
      "undo\n",
      "score_new 119.00000000000087 score_prev 122.06666666666739 gained -3.0666666666665208\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 122.06666666666739 gained -13.066666666666379\n",
      "undo\n",
      "score_new 103.80000000000094 score_prev 122.06666666666739 gained -18.266666666666453\n",
      "undo\n",
      "score_new 108.86666666666765 score_prev 122.06666666666739 gained -13.199999999999733\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 122.06666666666739 gained -13.066666666666379\n",
      "undo\n",
      "score_new 96.46666666666749 score_prev 122.06666666666739 gained -25.599999999999895\n",
      "undo\n",
      "score_new 113.00000000000107 score_prev 122.06666666666739 gained -9.066666666666322\n",
      "undo\n",
      "score_new 103.53333333333426 score_prev 122.06666666666739 gained -18.533333333333132\n",
      "undo\n",
      "score_new 109.00000000000101 score_prev 122.06666666666739 gained -13.066666666666379\n",
      "undo\n",
      "score_new 112.46666666666772 score_prev 122.06666666666739 gained -9.599999999999667\n",
      "undo\n",
      "score_new 119.13333333333422 score_prev 122.06666666666739 gained -2.9333333333331666\n",
      "undo\n",
      "result rel_coord_default previous_gain: nan per tacts: nan duration 0 days 00:02:31.289467\n",
      "WRITTEN\n",
      "opt# 2\n",
      "scores for optimizers augmented tensor([-4.6445e+01,  3.1044e+01,  2.5708e+01,  1.0000e+10, -4.0000e-02,\n",
      "         1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10])\n",
      "chosen evol_soft previous_result: nan per tacts: nan\n",
      "iteration 0 y= tensor([126.4667, 108.3333,  97.0000])\n",
      "iteration 1 y= tensor([127.2667, 122.8667, 122.4667])\n",
      "iteration 2 y= tensor([104.3333,  98.0667,  97.0000])\n",
      "iteration 3 y= tensor([127.1333, 123.9333, 114.3333])\n",
      "iteration 4 y= tensor([125.2667, 123.6667, 118.8667])\n",
      "iteration final y= tensor([123.5333, 122.0667, 121.5333])\n",
      "result evol_soft previous_gain: nan per tacts: nan duration 0 days 00:08:03.824470\n",
      "WRITTEN\n",
      "opt# 3\n",
      "random trial\n",
      "scores for optimizers augmented tensor([-4.4509e+01,  3.1393e+01,  2.8132e+01,  4.3091e+00,  9.4188e-01,\n",
      "         1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10])\n",
      "chosen gradient_slow_20 previous_result: nan per tacts: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.function(opt.best_genoms[-1])\n",
    "\n",
    "for i in range(1000):\n",
    "    print('opt#',i)\n",
    "    opt.optimize()\n",
    "    with open('genoms/best_genom_tetris.pkl', 'wb') as f:\n",
    "        pickle.dump(opt.best_genoms,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('WRITTEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./genoms/genom1.pkl', 'rb') as f:\n",
    "        genom = pickle.load(f)\n",
    "        #genom = genom[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBhHJSL-ctuu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= 90.86666666666578\n"
     ]
    }
   ],
   "source": [
    "q=multy_test(genom,draw=True)\n",
    "print('q=',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rquJJEbVcfgz"
   },
   "outputs": [],
   "source": [
    "video[0].save(\n",
    "    './out_videos/tetris.gif',\n",
    "    save_all=True,\n",
    "    append_images=video[1:], \n",
    "    optimize=True,\n",
    "    duration=100,\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise stategy\n",
    "#with open('genoms/best_genom_tetris.pkl', 'rb') as f:\n",
    "    #genom = pickle.load(f)[-1]\n",
    "globals()['video'] = []\n",
    "noise = np.random.normal(size=[len(genom)])*1\n",
    "for i in range(1):\n",
    "    genom = genom+noise\n",
    "    q=test_atary(genom, draw=True,game_name=\"ALE/Tetris-v5\",seed=0)\n",
    "    np.random.seed(i)\n",
    "    noise = np.random.normal(size=[len(genom)])*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NolxDC4bdi7r"
   },
   "outputs": [],
   "source": [
    "video[0].save(\n",
    "    './out_videos/tetris.gif',\n",
    "    save_all=True,\n",
    "    append_images=video[1:], \n",
    "    optimize=True,\n",
    "    duration=10,\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genoms/best_genom_tetris.pkl', 'wb') as f:\n",
    "    pickle.dump(genom,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('WRITTEN')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1/6tnER+ipw9TdwDyZ+sz",
   "collapsed_sections": [],
   "mount_file_id": "1LelKGSZ7B8o8qafqCX4oFBtzHByVJ9FI",
   "name": "atari_experiment2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
